{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "358cd747",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from utils.conversion import ms_to_numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5bb284",
   "metadata": {},
   "source": [
    "# Inferring selection strength\n",
    "\n",
    "Using the method of [Messer & Neher (2012)](https://doi.org/10.1534/genetics.112.138461)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc3dde3",
   "metadata": {},
   "source": [
    "## Empirical window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9064a5ad",
   "metadata": {},
   "source": [
    "We will attempt to get $L(\\mu + r)/s \\approx 0.1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "925b6d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_s = 0.01\n",
    "max_s = 1.0\n",
    "mut_rate = snakemake.params[\"mut_rate\"]\n",
    "rec_rate = snakemake.params[\"rec_rate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db4cb720",
   "metadata": {},
   "outputs": [],
   "source": [
    "L_min = int((0.1*min_s)/(mut_rate + rec_rate))\n",
    "L_max = int((0.1*max_s)/(mut_rate + rec_rate))\n",
    "print(f'To get u/s = 0.1 with s={min_s}, we need L={L_min}')\n",
    "print(f'To get u/s = 0.1 with s={max_s}, we need L={L_max}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c08ca6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = int(snakemake.wildcards[\"window_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5e32b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "u = (window_size*(mut_rate + rec_rate))\n",
    "print(f\"With a window size (L) of {window_size}, we get u={u}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e27d04f",
   "metadata": {},
   "source": [
    "Get a window in the center of the region:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d70f418f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(snakemake.input[\"ms\"]) as f:\n",
    "    positions, haplotypes = ms_to_numpy(f)\n",
    "region_center = (positions.min() + positions.max())/2\n",
    "win_start = region_center - window_size/2\n",
    "win_end = region_center + window_size/2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ebaf20",
   "metadata": {},
   "source": [
    "Subset the haplotypes to the window:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dafe5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.where((positions > win_start) & (positions < win_end))[0]\n",
    "subpositions = positions[indices]\n",
    "subwindow = haplotypes[indices, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfd3b67",
   "metadata": {},
   "source": [
    "## Haplotype frequency spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bff17c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "hap_strings = [\"\".join(str(i) for i in hap) for hap in subwindow.T]\n",
    "hap_counts = Counter(hap_strings)\n",
    "haps_by_frequency = sorted(hap_counts, key=lambda x: hap_counts[x], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cd620b",
   "metadata": {},
   "source": [
    "How many unique haplotypes are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1417df74",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hap_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4953a90b",
   "metadata": {},
   "source": [
    "Plot the haplotype distance matrix, ordered by most common haplotype:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9613789b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hap_arrays_by_frequency = [list(hap) for hap in haps_by_frequency]\n",
    "prop_diff_sites = pdist(\n",
    "    hap_arrays_by_frequency,\n",
    "    metric=\"hamming\"\n",
    ")\n",
    "diff_sites = squareform(prop_diff_sites*len(subpositions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "976bd512",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(diff_sites)\n",
    "cbar = plt.colorbar(im)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14ab911",
   "metadata": {},
   "source": [
    "## Split into sweep components\n",
    "\n",
    "Perform clustering to see if there are groups of related haplotypes. I moved the parameter $t$ around such that we have sensical clusters of haplotypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30e4cb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster until no single cluster has only 1 haplotype\n",
    "clustering = linkage(prop_diff_sites, method='single')\n",
    "min_hap = 1\n",
    "t = 0\n",
    "iteration = 0\n",
    "while min_hap == 1:\n",
    "    iteration += 1\n",
    "    t += 0.01\n",
    "    hap_clusters = fcluster(clustering, t=t, criterion='distance')\n",
    "    clust_info = np.unique(hap_clusters, return_counts=True)\n",
    "    min_hap = min(clust_info[1])\n",
    "    \n",
    "print(f\"Clustering iteration {iteration} with t={t}\\n\")\n",
    "for ix, clust in enumerate(clust_info[0]):\n",
    "    print(f\"Cluster {clust}: {clust_info[1][ix]} haplotypes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfd07b2",
   "metadata": {},
   "source": [
    "Order haplotypes by their cluster, then by their frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2167ab7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered = defaultdict(list)\n",
    "for ix, hap in enumerate(haps_by_frequency):\n",
    "    cluster = hap_clusters[ix]\n",
    "    ordered[cluster].append(hap)\n",
    "    \n",
    "haps_by_cluster = []\n",
    "for cluster_ix, haps in ordered.items():\n",
    "    haps_by_cluster += haps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "037b1373",
   "metadata": {},
   "outputs": [],
   "source": [
    "hap_arrays_by_cluster = [list(hap) for hap in haps_by_cluster]\n",
    "prop_diff_sites_cluster = pdist(\n",
    "    hap_arrays_by_cluster,\n",
    "    metric=\"hamming\"\n",
    ")\n",
    "diff_sites = squareform(prop_diff_sites_cluster*len(subpositions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c04770a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(diff_sites)\n",
    "cbar = plt.colorbar(im)\n",
    "plt.savefig(snakemake.output[\"clusters\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c055665",
   "metadata": {},
   "source": [
    "## Estimate selection strength\n",
    "\n",
    "We will estimate one $s$ per cluster, even though some clusters might just be neutral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50d4cdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many different haplotypes do we consider per cluster?\n",
    "num_most_abundant_unique_haps = int(snakemake.wildcards[\"haps_per_cluster\"])\n",
    "\n",
    "u = window_size*(mut_rate + rec_rate)\n",
    "\n",
    "result_dfs = []\n",
    "\n",
    "with open(snakemake.output[\"estimate\"], 'w') as f:\n",
    "    f.write(f\"Window size: {window_size}\" + '\\n')\n",
    "    f.write(f\"Haplotypes per cluster: {num_most_abundant_unique_haps}\" + '\\n\\n')\n",
    "    for cluster, haps in ordered.items():\n",
    "\n",
    "        cluster_name = f'Cluster {cluster}'\n",
    "        f.write(f'{cluster_name}\\n' + '-'*len(cluster_name) + '\\n\\n')\n",
    "\n",
    "        haps_subset = Counter({hap: hap_counts[hap] for hap in haps})\n",
    "        most_common = haps_subset.most_common(num_most_abundant_unique_haps)\n",
    "\n",
    "        for hap, count in most_common:\n",
    "            hap_string = hap\n",
    "            if len(hap_string) > 35:\n",
    "                hap_string = hap_string[:32] + '...'\n",
    "            f.write(hap_string + '\\t' + 'x' + str(count) + '\\n')\n",
    "        f.write('\\n')\n",
    "        \n",
    "        counts = sorted([duo[1] for duo in most_common], reverse=True)\n",
    "\n",
    "        n_0 = counts[0]\n",
    "        n_c = min(counts)\n",
    "        i_c = len(counts)\n",
    "\n",
    "        s = (u/i_c)*(n_0/n_c)**(1 + (n_c*i_c)/n_0)\n",
    "        error = s*(1/np.sqrt(i_c))\n",
    "        \n",
    "        f.write(f\"i_c = {i_c} haplotypes with an abundance of at least n_c = {n_c}.\\n\")\n",
    "        if i_c < num_most_abundant_unique_haps:\n",
    "            f.write(f\"This cluster doesn't have at least {num_most_abundant_unique_haps} haplotypes.\")\n",
    "        elif n_c == 1:\n",
    "            f.write(\"This cluster is probably neutral.\\n\")\n",
    "        else:\n",
    "            f.write(f\"Estimated s = {s} +/- {error}\\n\")\n",
    "            this_df = pd.DataFrame({\n",
    "                'estimated_s': s,\n",
    "                'estimated_error': error,\n",
    "                'cluster': cluster,\n",
    "                'counts': counts\n",
    "            })\n",
    "            result_dfs.append(this_df)\n",
    "    \n",
    "        f.write('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01d9ce86",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df = pd.concat(result_dfs)\n",
    "except ValueError: # no good clusters at all\n",
    "    df = pd.DataFrame({'cluster': []})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5744e78",
   "metadata": {},
   "source": [
    "## Plot haplotype frequency spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "695157b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "u = window_size*(mut_rate + rec_rate)\n",
    "\n",
    "outfolder = Path(snakemake.output[\"hfs_folder\"])\n",
    "outfolder.mkdir(exist_ok=True)\n",
    "\n",
    "for cluster in df.cluster.unique():\n",
    "    \n",
    "    data = df.loc[df.cluster == cluster]    \n",
    "    y = data.counts\n",
    "    s = data.estimated_s[0]\n",
    "    i = np.arange(1, len(y))\n",
    "    beta = 1 - (u/s)\n",
    "    expected_fracs = [1] + list((u/(i*s))**beta)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(range(len(y)), y/y[0], label='Observed')\n",
    "    ax.scatter(range(len(y)), y/y[0])\n",
    "    ax.plot(range(len(y)), expected_fracs, label='Expected')\n",
    "    ax.scatter(range(len(y)), expected_fracs)\n",
    "    ax.set_yscale('log')\n",
    "    ax.legend()\n",
    "    ax.set_xlabel('Haplotype rank i')\n",
    "    ax.set_ylabel('n_i/n_0')\n",
    "    ax.set_title(f\"HFS for cluster {cluster}\")\n",
    "    \n",
    "    plt.savefig(outfolder/f\"cluster-{cluster}.pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68503c99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
